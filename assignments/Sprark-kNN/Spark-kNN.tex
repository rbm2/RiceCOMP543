%Copyright © 2019  Christopher M Jermaine (cmj4@rice.edu), and Risa B Myers  (rbm2@rice.edu)
%
%Licensed under the Apache License, Version 2.0 (the "License");
%you may not use this file except in compliance with the License.
%You may obtain a copy of the License at
%
%    https://www.apache.org/licenses/LICENSE-2.0
%
%Unless required by applicable law or agreed to in writing, software
%distributed under the License is distributed on an "AS IS" BASIS,
%WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%See the License for the specific language governing permissions and
%limitations under the License.
\documentclass[11pt]{article}
%\documentclass[12pt]{amsart}
%\usepackage{latex8}
\usepackage{fullpage}
\usepackage{times}
\usepackage{url}
\usepackage[normalem]{ulem}
\usepackage{epsfig} 
%\usepackage{latexsym}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{multirow}

\titlespacing{\section}{0pt}{3mm}{1mm}
\titlespacing{\subsection}{0pt}{2mm}{0.5mm}
\titlespacing{\subsubsection}{0pt}{2mm}{0.8mm}

%\topmargin 0.75in 
%\oddsidemargin -0.04in
%\textwidth 6.5in
%\textheight 9.0in 
%\setlength{\textheight}{23.1cm}
%\setlength{\textwidth}{17.0cm}

\newcommand{\muhat}{\hat{\mu}}
\newcommand{\sigmahat}{\hat{\sigma}}
\newcommand{\todo}[1]{[\textbf{TODO: #1}]}
\newcommand{\eat}[1]{} % TO MAKE LARGE BLOCKS OF TEXT INVISIBLE
\newcommand{\sz}[1]{\lvert#1\rvert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\xp}[2]{P \if*#1\else^{#1}\fi \if*#2\else_{\! #2}\fi}
\newcommand{\pr}[3]{\xp{#1}{#2}\left\{\,#3\,\right\}}
\newcommand{\prl}[3]{\xp{#1}{#2}\{\,#3\,\}}
\renewcommand\:{\colon} % for use with \sset, etc.
\newcommand{\sset}[1]{\left\{\,#1\,\right\}}
\newcommand\xD{\mathcal{D}}
\newcommand\xP{\mathcal{P}}
\newcommand\xS{\mathcal{S}}
\newcommand\xbar{\bar x}
\newcommand\vbar{\bar v}
\newcommand\xmax{{x_{\text{max}}}}
\newcommand\eps{\epsilon}
\newcommand{\eeblk}{\hbox{\lower 1pt \vbox{\hrule width6pt\hbox to
  6pt{\vrule height5pt depth1pt \hfil\vrule height5pt depth1pt} \hrule
  width6pt} \unskip}}
\newcommand{\eblk}{{\unskip\nobreak\hfil\penalty50
  \hskip1em\hbox{}\nobreak\hfil\eeblk
  \parfillskip=0pt\finalhyphendemerits=0\par}}
\newtheorem{xample}{Example}
%\newenvironment{example}{\begin{xample}\em}{\eblk\end{xample}}
\makeatletter
\newenvironment{sql}%
 {\vskip 5pt\begin{list}{}{%
  \setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}\setlength{\parskip}{0pt}%
  \setlength{\parsep}{0pt}\setlength{\labelwidth}{0pt}%
  \setlength{\rightmargin}{0pt}\setlength{\leftmargin}{0pt}%
  \setlength{\labelsep}{0pt}%
  \obeylines\@vobeyspaces\normalfont\ttfamily%
  \item[]}}
 {\end{list}\vskip5pt\noindent}
\makeatother
\newcommand{\bpar}[1]{\vskip 5pt\noindent\textbf{#1}\hskip 1em}
\newcommand\yN{{\tilde N}}
\newcommand\yX{{\tilde X}}
\newcommand\ymu{{\tilde\mu}}
\newcommand\ysigma{{\tilde\sigma}}


\newcommand{\goodgap}{
        \hspace{\subfigtopskip}
        \hspace{\subfigbottomskip}
}

%\renewcommand{\baselinestretch}{0.99}

\newtheorem{definition}{Definition}
\newtheorem{Rule}{Rule}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{optimization}{Optimization}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}

\newcommand{\qed}{\hspace*{\fill}
           \vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}\smallskip}

\long\def \ignoreme#1{}

\def\qed{\hfill \mbox{\rule[0pt]{1.5ex}{1.5ex}}}



\begin{document}
%\maketitle
%\pagestyle{empty}

\begin{center}
{\bf \huge{Data Science Tools and Models: Spark kNN}}
\end{center}


\vspace{10 pt}

\section{Description}

In this assignment, you will be implementing a kNN classifier to classify text documents. ``Classification'' is the task of labeling documents based upon their contents. The implementation will be in Python, on top of Spark, \textbf{USING RDDs}. You will be asked to perform three subtasks, covering data preparation and classification.

\section{Data}
 
You will be dealing with MEDLINE/PubMed data:\\
 (\url{https://www.nlm.nih.gov/bsd/pmresources.html}). \\
 
 MEDLINE is a bibliographic database of journal citations, abstracts, and metadata.
 The data set is composed of categories,  identifier, and  abstract triples. This data set has 26,754 such posts from 11 different categories, according to catagories assigned to the articles. The 11 categories are listed in the file \texttt{categories.txt}. 
%The format of the text data set is exactly the same as the text format used for the documents in A5; 
The category name can be extracted from the name of the document. For example, the document with identifier \texttt{doc id=Wounds/24458063} is from the \texttt{Wounds} category. The document with the identifier \texttt{doc id=MedicalEducation/20662575} is from the \texttt{MedicalEducation} category. The data file has one line per document for a total of \textasciitilde 38 MB of text. It can be accessed via Spark at:\\
\texttt{s3://[DataLocation]/pubmed.txt}

A small subset of the data, for testing purposes, is in \texttt{pubmedSmall.txt}.

\section{The Tasks}
There are three separate tasks that you need to complete to finish the assignment.

\subsection{Task 1}

First, you need to write Spark code that builds a dictionary that includes the 20,000 most frequent words in the training corpus - this was part of the Spark Lab. When you do this, please start with the code provided with THIS assignment in \texttt{kNNstart.py} so that we know that everyone has the same dictionary. Then, you need to use this dictionary to create an RDD where each document is represented as one entry in the RDD. Specifically, the key of the document is the document identifier (like \texttt{BrainInjuries/17626737}) and the value is a NumPy array with 20,000 entries, where the $i$th entry in the array is the number of times that the $i$th word in the dictionary appears in the document.

Once you do this, print out the arrays that you have created for documents

\texttt{Wounds/23778438}, 

\texttt{ParasiticDisease/2617030}, and

\texttt{RxInteractions/1966748}

Since each array is going to be huge, with a lot of zeros, the thing that you want to print out is just the non-zero entries in the array (that is, for an array \texttt{a}, print out \texttt{a[a.nonzero ()]}.
\subsection{Task 2}
It is often difficult to classify documents accurately using raw count vectors. Thus, the next task is to write some more Spark code that converts each of those 26,754  count vectors to TF-IDF vectors ``term frequency-inverse document frequency vectors''). The $i$th entry in a TF-IDF vector for document $d$ is computed as:\\
\begin{center}
$TF(i,d) \times IDF(i)$\\
\end{center}

Where $TF(i,d)$ is:\\
\begin{center}
$\frac{\textrm{Number of occurrences of word } \textit{i} \textrm{ in } \textit{d}}{\textrm{Total number of words in } \textit{d}}$
\end{center}

Note that the ``Total number of words'' is not the number of distinct words. The ``total number of words'' in ``Today is a great day today'' is six. And the $IDF(i)$ is:\\

\begin{center}
$\textrm{log}\frac{\textrm{Size of corpus (number of docs)}}{\textrm{Number of documents having word } \textit{i}}$
\end{center}


Again, once you do this, print out the arrays that you have created for documents:\\

\texttt{PalliativeCare/16552238},

\texttt{SquamousCellCancer/23991972} and

\texttt{HeartFailure/25940075}\\ 

Again, print out just the non-zero entries.

\subsection{Task 3}
Next, your task is to build a pySpark kNN classifier, embodied by the Python function \texttt{predictLabel}. This function will take as input a text string and a number $k$, and then output the name of one of the 11 categories. This name is the new group that the classifier thinks that the text string is ``closest'' to. It is computed using the classical kNN algorithm. This algorithm first converts the input string into a TF-IDF vector (using the dictionary and count information computed over the original corpus). It then finds the $k$ documents in the corpus that are ``closest'' to the query vector (where distance is computed using the $L_2$ norm), and returns the category label that is most frequent in those top $k$. Ties go to the label with the closest corpus document. Once you have written your function, run it on the following (each is an excerpt from a pubmed abstract, chosen to match one of the 11 categories).  These function call are provided in the file  \texttt{kNNQueries.py}.\\
\section{Turn in}
Submit two documents: one with your results (.txt) and one with your code (.py).
Create a single document that has results for all three tasks. Turn in this document as well as all of your code. Please zip up all of your code and your text document (use .gz or .zip only, please!), or else attach each piece of code as well as your text results document to your submission individually. No PDFs of code, please!
\section{Grading}
Each task is worth 33\% of the overall grade.
\end{document}
