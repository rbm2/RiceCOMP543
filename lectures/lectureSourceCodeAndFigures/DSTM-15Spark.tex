%Copyright 2019 Christopher M. Jermaine (cmj4@rice.edu) and Risa B. Myers (rbm2@rice.edu)
%
%Licensed under the Apache License, Version 2.0 (the "License");
%you may not use this file except in compliance with the License.
%You may obtain a copy of the License at
%
%    https://www.apache.org/licenses/LICENSE-2.0
%
%Unless required by applicable law or agreed to in writing, software
%distributed under the License is distributed on an "AS IS" BASIS,
%WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%See the License for the specific language governing permissions and
%limitations under the License.
%===============================================================
\documentclass[aspectratio=169]{beamer}
\mode<presentation>
{
\usetheme[noshadow, minimal,numbers,riceb,nonav]{Rice}
\usefonttheme[onlymath]{serif}
\setbeamercovered{transparent}
}
\useinnertheme{rectangles}

\usepackage[english]{babel}

\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{trajan}
\usepackage{ textcomp }

% for hyperlinks
\usepackage{hyperref}

\usepackage{listings}

\newenvironment{noindentitemize}
{ \begin{itemize}
 \setlength{\itemsep}{1.5ex}
  \setlength{\parsep}{0pt}   
  \setlength{\parskip}{0pt}
 \addtolength{\leftskip}{-2em}
 }
{ \end{itemize} }

\newenvironment{noindentitemize2}
{ \begin{itemize}
  \setlength{\itemsep}{0ex}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}   
  \addtolength{\leftskip}{-2em}  }
{ \end{itemize} }



\lstnewenvironment{SQL}
  {\lstset{
        aboveskip=5pt,
        belowskip=5pt,
        escapechar=!,
        mathescape=true,
        upquote=true,
        language=SQL,
        basicstyle=\linespread{0.94}\ttfamily\footnotesize,
        morekeywords={PRINT, CURSOR, OPEN, FETCH, CLOSE, DECLARE, BEGIN, END, PROCEDURE, FOR, EACH, WITH, PARTITION, 	TEST, WHETHER, PROBABILITY, OUT,LOOP,IF,CONTINUE, HANDLER,CALL, FUNCTION, RETURNS, LANGUAGE,BODY,RETURN, REPLACE,plpgsql,
        RAISE, NOTICE,
        REPLACE, ROW, BEFORE, EXIT, TEXT, REFCURSOR, QUOTE_LITERAL, DELIMITER,CONCAT,FOUND,LEAVE },
        deletekeywords={VALUE, PRIOR},
        showstringspaces=true}
        \vspace{0pt}%
        \noindent\minipage{0.65\textwidth}}
  {\endminipage\vspace{0pt}}
  
  
\lstnewenvironment{SQLtiny}
  {\lstset{
        aboveskip=5pt,
        belowskip=5pt,
        escapechar=!,
        mathescape=true,
        upquote=true,
        language=SQL,
        basicstyle=\linespread{0.94}\ttfamily\tiny,
        morekeywords={PRINT, CURSOR, OPEN, FETCH, CLOSE, DECLARE, BEGIN, END, PROCEDURE, FOR, EACH, WITH, PARTITION, 	TEST, WHETHER, PROBABILITY, OUT,LOOP,IF,CONTINUE, HANDLER,CALL, FUNCTION, RETURNS, LANGUAGE,BODY,RETURN, REPLACE,plpgsql,
        RAISE, NOTICE,
        REPLACE, ROW, BEFORE, EXIT, TEXT, REFCURSOR, QUOTE_LITERAL, DELIMITER,CONCAT,FOUND,LEAVE },
       deletekeywords={VALUE, PRIOR},
        showstringspaces=true}
        \vspace{0pt}%
        \noindent\minipage{0.47\textwidth}}
  {\endminipage\vspace{0pt}}

%===============================================================%

\title[]
{Tools \& Models for Data Science}

\subtitle{Big Data Part Two: Beyond MapReduce}

\author[]{Chris Jermaine \& Risa Myers}
\institute
{
  Rice University 
}

\date[]{}

\subject{Beamer}


\begin{document}

\begin{frame}
 \titlepage
\end{frame}

%***********************************************************
\begin{frame}{Most Popular ``Pure'' MapReduce Software}

\begin{itemize}
\item Is called Hadoop
\begin{itemize}
\item Runs on JVM (like most Big Data software)
\item Includes MapReduce functionality
\item Plus the Hadoop distributed file system (HDFS)
\end{itemize}
\item Hadoop popularity peaked around 2015...
\item Has been declining since then
\item Why?  Were several issues...
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}{Many Felt MapReduce Too Slow}

\begin{itemize}
\item Data reread from DFS for each MR job
\item Bad for iterative data processing
	\begin{itemize}
	\item Example: most machine learning uses gradient descent
	\item Need to make 100's or 1000's of passes over data
	\item Re-evaluating gradient at various points
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}{MR API is Too Restrictive}

\begin{itemize}
\item Can only do Map
\item Or MapReduce
\item Everything else must be implemented in terms of those operations
\item Unless you cheat 
	\begin{itemize}
	\item Ex: Mappers/Reducers talk to each other using sockets
	\item But then why not just go with C/MPI? % message passing interface
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}{Result: MapReduce Used Less and Less}

\begin{itemize}
\item There are now an entire ecosystem of alternative softwares
	\begin{itemize}
	\item Spark, Flink, etc. 
	% Flink = Apache stream processing software
	\end{itemize}
\item For use in both streaming and batch processing applications
\item Generally oriented more towards in-memory computing
\item Have far more expressive APIs
\item We will focus on Spark
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}{Apache Spark}

\begin{itemize}
\item \#1 Hadoop MapReduce killer
\item What is Spark?
	\begin{itemize}
	\item Platform for efficient distributed data analytics
	\end{itemize}
\item Runs on the JVM
\item Written in Scala
	\begin{itemize}
	\item But has bindings for Java, Scala, Python, R
	\item Python nice for data analytics (NumPy, SciPy)... will focus there
	\end{itemize}
\item Doesn't do storage
	\begin{itemize}
	\item Focus exclusively on compute
	\item Commonly used with HDFS, S3, HBase, etc.
	\end{itemize}
\end{itemize}
\end{frame}


%***********************************************************

%\begin{frame}{Apache Spark Architecture}
%\begin{columns}[T]
%\begin{column}{0.65\textwidth}
%\includegraphics[width=1\textwidth]{lect11/cluster-overview.png}%
%\end{column}
%\begin{column}{0.35\textwidth}
%\begin{itemize}
%\item SparkContext - Coordinator
%\item Cluster Manager 
%\item Executor - Assigned to an application 
%\end{itemize}
%\end{column}
%\end{columns}
%https://spark.apache.org/docs/latest/cluster-overview.html
%\end{frame}

%***********************************************************

\begin{frame}{RDDs}

\begin{itemize}
\item Basic abstraction: Resilient Distributed Data Set (RDD)
\item {\textcolor{red}{Resilient}} - fault tolerance
\item {\textcolor{red}{Distributed}} - across machines in a cluster
\item {\textcolor{red}{Data Set}} - ``working set of data''
\item Read-only
\item RDD is a data set buffered in RAM by Spark
\end{itemize}

\end{frame}


%***********************************************************

\begin{frame}{More about RDDs \& pySpark}

\begin{itemize}
\item pySpark is interactive!
\item RDDs
\begin{itemize}
\item Are lazy: computations are done on demand and only as much as needed
\item Are ephemeral: computations are discarded when no longer used
\item Have lineage: the train of computations to generate an RDD is saved and may be replayed
\end{itemize}
\end{itemize}

\end{frame}

%***********************************************************

\begin{frame}[fragile]{Creating RDDs}

\begin{itemize}
\item To create and load an RDD:
\end{itemize}

\begin{SQL}
data = [1, 2, 3, 4, 5]
myRDD = sc.parallelize (data) 
\end{SQL}

\begin{itemize}
\item Try it
\end{itemize}

\end{frame}

%***********************************************************

\begin{frame}[fragile]{Converting an RDD to a List}

\begin{itemize}
\item collect
\begin{itemize}
\item Be careful - this returns the ENTIRE RDD
\end{itemize}
\item top(n) 
\begin{itemize}
\item Based on implicit ordering
\end{itemize}
\item take(n) 
\begin{itemize}
\item First $n$ elements
\end{itemize}
\end{itemize}

\begin{SQL}
myRDD.collect()
myRDD.top(3)
myRDD.take(2)
\end{SQL}

\begin{itemize}
\item Try it
\end{itemize}

\end{frame}

%***********************************************************

\begin{frame}[fragile]{Create an RDD from a Range}

\begin{itemize}
\item To create and load an RDD:
\end{itemize}

\begin{SQL}
myRDD = sc.parallelize (range (20000)) 
\end{SQL}

\begin{itemize}
\item Try it
\end{itemize}


\end{frame}
%***********************************************************

\begin{frame}[fragile]{Other ways to create an RDD}

\begin{itemize}
\item To create and load an RDD from a file:
\end{itemize}

\begin{SQL}
myRDD = sc.textFile (someFileName) # sc is the Spark context
\end{SQL}

\begin{itemize}
\item Try it
\end{itemize}


\end{frame}


%***********************************************************

\begin{frame}[fragile]{Create an RDD from Another RDD}

\begin{itemize}
\item Computations: Series of Xforms Over RDDs
\item Example: word count
\end{itemize}

\begin{SQL}
def countWords (fileName):
     lines = sc.textFile (fileName)
     tokens = lines.flatMap (lambda line: line.split(" "))
     instances = tokens.map (lambda word: (word, 1))
     aggCounts = instances.reduceByKey (lambda a, b: a + b)
     return aggCounts.top (200, key=lambda p: p[1])
\end{SQL}

\begin{itemize}
\item What transforms do we see here?
	\begin{itemize}
	\item flatMap, map, reduceByKey, top
	\end{itemize}
\item Let's go through them
\end{itemize}
\end{frame}

%***********************************************************

\begin{frame}[fragile]{Lambdas}

\begin{itemize}
\item But first, quick review of lambdas...
	\begin{itemize}
        \item Fundamental to programming in Spark
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}[fragile]{What's a Lambda?}

\begin{itemize}
\item Basically, a function that that we can pass like a variable
\item Key ability: can ``capture'' its surroundings at creation
\item Can also accept parameters
\end{itemize}

\begin{SQL}
def addTwelveToResult (myLambda):
     return myLambda (3) + 12

a = 23
aCoolLambda = lambda x : x + a
addTwelveToResult (aCoolLambda) # prints 38

a = 45
addTwelveToResult (aCoolLambda) # prints ??? 
\end{SQL}
% 60
\begin{itemize}
\item Try it
\end{itemize}
\end{frame}

%***********************************************************

\begin{frame}[fragile]{What's going on here?}

\begin{itemize}
\item Anytime we see ``myLambda'' we replace it with the body of the lambda 
\item Kind of like a database VIEW
\end{itemize}

\begin{SQL}
def addTwelveToResult (myLambda):
     return myLambda (3) + 12

a = 23
aCoolLambda = lambda x : x + a
addTwelveToResult (aCoolLambda) 
\end{SQL}

\begin{itemize}
\item Results in 
\end{itemize}

\begin{SQL}
myLambda (3) + 12
(x + a) + 12
(3 + a) + 12
(3 + 23) + 12 
\end{SQL}

\begin{itemize}
\item The parentheses matter!
\end{itemize}


\end{frame}

%***********************************************************

\begin{frame}[fragile]{Lambdas}

\begin{itemize}
\item Lambdas can return many items
\item Lambdas MUST return something
\end{itemize}

\begin{SQL}
def sumThem (myLambda):
     tot = 0
     for a in myLambda ():
          tot = tot + a
     return tot

x = np.array([1, 2, 3, 4, 5])
iter = lambda : (j for j in x)
sumThem (iter) # prints 15
\end{SQL}
\end{frame}
%***********************************************************

\begin{frame}[fragile]{Spark Operation: flatMap ()}

\begin{SQL}
def countWords (fileName):
     lines = sc.textFile (fileName)
     tokens = lines.flatMap (lambda line: line.split(" "))
\end{SQL}

\begin{itemize}
\item Processes every data item in the RDD
\item Apply lambda to it
\item Lambda argument will return zero or more results
\item Can omit, combine or create elements
\item Each result added into resulting RDD
\end{itemize}
\end{frame}
%***********************************************************

\begin{frame}[fragile]{Spark Operation: map ()}

\begin{SQL}
def countWords (fileName):
     lines = sc.textFile (fileName)
     tokens = lines.flatMap (lambda line: line.split(" "))
     instances = tokens.map (lambda word: (word, 1))
\end{SQL}

\begin{itemize}
\item Process every data item in the RDD
\item Apply lambda to it
\item The lambda must return exactly one result
\item The returned RDD has a new element with each element replaced by the lam
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{Spark Operation: reduceByKey ()}

\begin{SQL}
def countWords (fileName):
     lines = sc.textFile (fileName)
     tokens = lines.flatMap (lambda line: line.split(" "))
     instances = tokens.map (lambda word: (word, 1))
     aggCounts = instances.reduceByKey (lambda a, b: a + b)
\end{SQL}

\begin{itemize}
\item Data must be $(Key, Value)$ pairs
\item Shuffle so that all $(K, V)$ pairs with same $K$ on same machine
\item Organize into $(K, (V_1, V_2, ..., V_n))$ pairs
\item Use the lambda to ``reduce'' the list to a single value
\item This is similar to our aggregate functions in SQL!
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{Spark Operation: top ()}

\begin{SQL}
def countWords (fileName):
     lines = sc.textFile (fileName)
     tokens = lines.flatMap (lambda line: line.split(" "))
     instances = tokens.map (lambda word: (word, 1))
     aggCounts = instances.reduceByKey (lambda a, b: a + b)
     return aggCounts.top (200, key=lambda p: p[1])
\end{SQL}

\begin{itemize}
\item Data must be $(Key, Value)$ pairs
\item Takes two params... first is the number of list items to return
\item Second (optional): lambda to use to obtain key for comparison
\item Note: \texttt{top} collects an RDD, moving from cloud to local
\item So result is \textbf{not} an RDD
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{An Important Note}

\begin{itemize}
\item Spark uses lazy evaluation...
\item If I run this code:
\end{itemize}

\begin{SQL}
lines = sc.textFile (fileName)
tokens = lines.flatMap (lambda line: line.split(" "))
instances = tokens.map (lambda word: (word, 1))
aggCounts = instances.reduceByKey (lambda a, b: a + b)
\end{SQL}

\begin{itemize}
\item Nothing happens! (Other than Spark remembers the ops)
	\begin{itemize}
	\item Spark does not execute until an attempt made to collect an RDD
	\item When we hit \texttt{top()}, then all of these are executed
	\end{itemize}
\item Why do this?
	\begin{itemize}
	\item By waiting until last possible second, we can  ``pipeline'' 
	\item Only operations that require a shuffle can't be pipelined
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{Some Other, More Advanced Ops}

\begin{itemize}
\item groupByKey(), join(), reduce(), aggregate()
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{groupByKey ()}

\begin{itemize}
\item Data must be $(Key, Value)$ pairs
\item Shuffle so that all $(K, V)$ pairs with same $K$ onto the same machine
\item Organize into $(K, \langle V_1, V_2, ..., V_n \rangle)$ pairs
\item Store each list as a $\texttt{ResultIterable}$ for future processing
\item Like $\texttt{reduceByKey()}$ but without the reduce
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{join ()}

\begin{itemize}
\item Given two data sets $rddOne$, $rddTwo$ of $(Key, Value)$ pairs
\item We join them using:
\end{itemize}

\begin{SQL}
rddOne.join (rddTwo)
\end{SQL}

\begin{itemize}
\item Returns $(K, (V_1, V_2))$ pairs
\item Constructed from all $(K_1, V_1)$ from $rddOne$, $(K_2, V_2)$ from $rddTwo$, where $K_1 = K_2$
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{join() (Continued)}

\begin{itemize}
\item Example:
	\begin{itemize}
	\item $rddOne$ is $\{(red, 9), (blue, 7), (red, 12), (green, 4)\}$
	\item $rddTwo$ is $\{(blue, up), (green, down), (green, behind)\}$
	\item Result of join is $\{(blue, (7, up)), (green, (4, down)), (green, (4, behind))\}$
	\end{itemize}
\item Can blow up RDD size if join is many-to-many
\item Requires expensive shuffle!
\item[?] How do we write the join?
\end{itemize}
\end{frame}

%rddOne = {('red', 9), ('blue', 7), ('red', 12), ('green', 4)}
%myRddOne = sc.parallelize (rddOne)
%
%rddTwo = {('blue', 'up'), ('green', 'down'), ('green', 'behind')}
%myRddTwo = sc.parallelize (rddTwo)
%
%myJoin = myRddOne.join(myRddTwo)
%[('blue', (7, 'up')), ('green', (4, 'behind')), ('green', (4, 'down'))]
%***********************************************************
\begin{frame}[fragile]{reduce()}

\begin{itemize}
\item Unlike past operations, this is not a transform from RDD to RDD
	\begin{itemize}
	\item This is like $\texttt{top()}$
	\item It moves the result back to Python
	\end{itemize}
\item Repeatedly applies a lambda to each item in the RDD to get single result
\end{itemize}

\begin{SQL}
>>> myData = sc.parallelize (range(20000))
>>> myData.reduce (lambda a, b: a + b)
199990000
\end{SQL}
\end{frame}
%***********************************************************
\begin{frame}{aggregate()}

\begin{itemize}
\item With $\texttt{reduce()}$ you aggregate directly, can be restrictive...
\item Example: RDD is $\{(red, 9), (blue, 7), (red, 12), (green, 4)\}$
	\begin{itemize}
	\item Want: \textbf{dictionary where value is sum for each unique color}
	\item Cannot use $\texttt{reduce()}$
	\item It only ``sums'' up the items in the input RDD directly
	\item Two inputs and output must be the same type
	\item How do I get the desired output dictionary by defining $+$ in $(((red, 9) + (blue, 7)) + (red, 12)) + (green, 4)$?
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{aggregate() (Continued)}

\begin{itemize}
\item Data must be $(Key, Value)$ pairs
\item Organize into $(K, \langle V_1, V_2, ..., V_n \rangle )$ pairs
\item Then aggregate the list, like $\texttt{reduce()}$
\item $\texttt{aggregate()}$ takes three arguments
	\begin{itemize}
        \item The ``zero'' to init the aggregation % this needs to be an empty object of the right structure - depending on what you are doing, it might be a dictionary, pair, etc.
	\item Lambda that takes $X_1$, $X_2$ and aggregates them, where $X_1$ already aggregated, $X_2$ not
	\item Lambda that takes $X_1$, $X_2$ and aggregates them, where both aggregated
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{aggregate() (Example)}

\begin{columns}[T]
\begin{column}{0.005\textwidth}
\begin{SQL}
1
2
3
4
5
6
7
8
9
\end{SQL}
\end{column}
\begin{column}{0.4\textwidth}
\begin{SQL}
def add (dict, tuple):
  result = {}
  for key in dict:
    result[key] = dict[key]
  if (tuple[0] in result):
    result[tuple[0]] += tuple[1]
  else:
    result[tuple[0]] = tuple[1]
  return result
\end{SQL}

Goal: Add tuple to dict and return a new dictionary
\end{column}
\begin{column}{0.555\textwidth}
\begin{enumerate}
\item Define add that takes a dictionary and a tuple
\item initialize the result dictionary
\item For each item in the dictionary we are given
\item \hspace{1em} Create an entry in our result dictionary with dict's value
\item If we've seen the tuple key before
\item  \hspace{1em}Accumulate the value
\item If this is the first time we see the tuple key
\item  \hspace{1em} Create an entry for it and initialize with the value
\item Return our result dictionary
\end{enumerate}
\end{column}
\end{columns}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{aggregate() (Example)}

\begin{columns}[T]
\begin{column}{0.005\textwidth}
\begin{SQL}
1
2
3
4
5
6
7
8
9
10
\end{SQL}
\end{column}
\begin{column}{0.4\textwidth}
\begin{SQL}
def combine (dict1, dict2):
  result = {}
  for key in dict1:
    result[key] = dict1[key]
  for key in dict2:
    if (key in result):
      result[key] += dict2[key]
    else:
      result[key] = dict2[key]
  return result
\end{SQL}
Goal: Return a new dictionary that contains all the keys in  dict1 and dict2 with the total counts 

\end{column}
\begin{column}{0.555\textwidth}
\begin{enumerate}
\item Define combine that takes 2 dictionaries
\item initialize the result dictionary
\item For each item in dict1 that we are given
\item \hspace{1em} Create an entry in our result dictionary with dict1's value
\item For each key, value in dict2 that we are given
\item \hspace{1em} If we've seen the key before
\item  \hspace{2em}Accumulate the value
\item \hspace{1em} If this is the first time we see the tuple key
\item  \hspace{2em} Create an entry for it and initialize with the value
\item Return our result dictionary
\end{enumerate}
\end{column}
\end{columns}
\end{frame}
%***********************************************************
\begin{frame}[fragile]{aggregate() (Example)}

\begin{SQL}
>>> myRdd = sc.parallelize ([('red', 9), ('blue', 7), 
... ('red', 12), ('green', 4)])
>>> myRdd.aggregate ({}, lambda x, y: add (x, y), 
... lambda x, y: combine (x, y))

{'blue': 7, 'green': 4, 'red': 21}
\end{SQL}
\end{frame}
%***********************************************************
\begin{frame}{Closing Thoughts}

\begin{itemize}
\item When is Spark/MapReduce a better option than HPC?
	\begin{itemize}
	\item When your pipeline is heavily data-oriented
	\item Or when your compute is (relatively) loosely coupled
	\end{itemize}
\item Key benefits compared to HPC
	\begin{itemize}
	\item Built in fault tolerance
	\item Better support for BIG data 
	\item Much higher programmer productivity
	\end{itemize}
\item Will continue to take market share from HPC
	\begin{itemize}
	\item You see academic papers with both MPI, Spark implementations
	\item But not everything can move to Spark
	\end{itemize}
\end{itemize}
\end{frame}

%***********************************************************
\begin{frame}{Wrap up}
	
	\url{https://spark.apache.org/docs/latest/rdd-programming-guide.html}
	\vspace{2em}
	
	\url{https://spark.apache.org/docs/latest/api/python/}

\begin{itemize}
	\item[?] How can we use what we learned today?
	\vspace{2em}
	\item[?] What do we know now that we didn't know before?
\end{itemize}


\end{frame}



\end{document}
