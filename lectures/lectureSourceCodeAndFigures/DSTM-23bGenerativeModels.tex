%Copyright 2019 Christopher M. Jermaine (cmj4@rice.edu) and Risa B. Myers (rbm2@rice.edu)
%
%Licensed under the Apache License, Version 2.0 (the "License");
%you may not use this file except in compliance with the License.
%You may obtain a copy of the License at
%
%    https://www.apache.org/licenses/LICENSE-2.0
%
%Unless required by applicable law or agreed to in writing, software
%distributed under the License is distributed on an "AS IS" BASIS,
%WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%See the License for the specific language governing permissions and
%limitations under the License.
%===============================================================
\documentclass[aspectratio=169]{beamer}
\mode<presentation> 
{
\usetheme[noshadow, minimal,numbers,riceb,nonav]{Rice}
\usefonttheme[onlymath]{serif}
\setbeamercovered{transparent}
}
\useinnertheme{rectangles}

\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\usepackage{trajan}
\usepackage{ textcomp }
\usepackage{listings}

\newenvironment{noindentitemize}
{ \begin{itemize}
 \setlength{\itemsep}{1.5ex}
  \setlength{\parsep}{0pt}   
  \setlength{\parskip}{0pt}
 \addtolength{\leftskip}{-2em}
 }
{ \end{itemize} }

\newenvironment{noindentitemize2}
{ \begin{itemize}
  \setlength{\itemsep}{0ex}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}   
  \addtolength{\leftskip}{-2em}  }
{ \end{itemize} }

\lstnewenvironment{SQL}
  {\lstset{
        aboveskip=5pt,
        belowskip=5pt,
        escapechar=!,
        mathescape=true,
        upquote=true,
        language=SQL,
        basicstyle=\linespread{0.94}\ttfamily\footnotesize,
        morekeywords={WHILE, DO, END},
        deletekeywords={VALUE, PRIOR},
        showstringspaces=true}
        \vspace{0pt}%
        \noindent\minipage{0.47\textwidth}}
  {\endminipage\vspace{0pt}}
  
\newcommand{\LIKES}{\textrm{LIKES}} 
\newcommand{\FREQUENTS}{\textrm{FREQUENTS}} 
\newcommand{\SERVES}{\textrm{SERVES}} 
\newcommand{\CAFE}{\textrm{CAFE}} 
\newcommand{\COFFEE}{\textrm{COFFEE}} 
\newcommand{\DRINKER}{\textrm{DRINKER}} 
\newcommand{\ALLPEEPS}{\textrm{ALLPEEPS}} 
\newcommand{\ALLCOMBOS}{\textrm{ALLCOMBOS}} 


\setbeamerfont{block body}{size=\tiny}

%===============================================================%

\title[]
{Tools \& Models for Data Science}

\subtitle{Generative Models}

\author[]{Risa Myers}
\institute
{
  Rice University 
}

\date[]{}

\subject{Beamer}

\begin{document}

\begin{frame}
 \titlepage
\end{frame}
%***********************************************************
\begin{frame}{Can we learn hyperparameters?}

\begin{itemize}
	\item Yes!
	\item We can keep increasing the complexity of our models
	\item Stop when it's not helpful or too expensive
	\item Sometimes use ``non-informative'' hyperparameters
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{What is a Generative Model?}

\begin{itemize}
	\item A model of the joint probability $p(x,y)$ of observed data $x$ and labels $y$
	\item Bayes rule is used to determine $p(y|x)$
	\item The most likely label, $y$ is selected
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{If it's not Generative, what Might it be?}

\begin{itemize}
	\item Discriminative model
	\begin{itemize}
	\item Discriminative models model $p(y|x)$ directly
	\item They learn a mapping from $x$ to $y$
	\end{itemize}
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{Another Way to Think these Types}

\begin{itemize}
	\item Generative models describe how the data were generated, inferring which label is most likely to have generated the data: $p(x,y)$
	\item Discriminative models just assign a label based on the observed data: $p(y|x)$
\end{itemize}
\end{frame}

%***********************************************************
\begin{frame}{Models we have Covered}

\begin{columns}
\begin{column}{0.5\textwidth}
Generative
\begin{itemize}
	\item Gaussian mixture model
	\item Hidden Markov model
	\item Latent Dirichlet allocation
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
Discriminative
\begin{itemize}
	\item k-nearest neighbors
	\item Logistic regression
	\item Support Vector Machines
\end{itemize}
\end{column}
\end{columns}
\end{frame}

%***********************************************************
\begin{frame}{Back to Model Parameters and Distribution Hyperparameters}

\begin{itemize}
	\item What $\langle \alpha \rangle$ vector do we use for a Dirichlet distribution?
	\item We can learn this, or we can
	\begin{itemize}
		\item Choose $\langle \alpha \rangle$ to be uniform
		\item Choose $\langle \alpha \rangle$ to reflect the data we expect to see
	\end{itemize}

\end{itemize}
\end{frame}

%***********************************************************
\begin{frame}{Plate Notation}

\begin{itemize}
	\item Graphical representation of a graphical model
	\item Can be used to illustrate the variables and relationships between variables in a generative model
\end{itemize}
\end{frame}
%***********************************************************
\begin{frame}{Plate Notation for LDA}

\begin{columns}
\begin{column}{0.5\textwidth}	
	\includegraphics[width=1\textwidth]{lectGen/Latent_Dirichlet_allocation.pdf}
\end{column}
\begin{column}{0.5\textwidth}
{\footnotesize{
\begin{itemize}
	\item $\alpha$: parameter for the Dirichlet prior on the topics
	\item $\beta$: parameter for the Dirichlet prior on the words
	\item Outermost plate: variables associated with a single document
	\item $\theta_i$: topic distribution for document $i$
	\item $M$: number of documents
	\item $N_i$: number of words in document $i$
	\item $z_{ij}$: topic for $j$th word in document $i$
	\item $w_{ij}$: actual word used
\end{itemize}
}}
\end{column}
\end{columns}

	\footnote{\url{https://upload.wikimedia.org/wikipedia/commons/d/d3/Latent\_Dirichlet\_allocation.svg}}
	
	\footnote{\url{https://creativecommons.org/licenses/by-sa/4.0})}
\end{frame}
%***********************************************************
\begin{frame}{References}
\begin{enumerate}
\item	Ng AY, Jordan A. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems. 2002.
\item \url{https://en.wikipedia.org/wiki/Generative_model}
\item \url{https://en.wikipedia.org/wiki/Plate_notation}
\end{enumerate}
\end{frame}
%***********************************************************
\begin{frame}{Questions?}
\begin{itemize}
	\item What do we know now that we didn't know before?
\begin{itemize}
	\item We know about two types of models: Generative and Discriminative
	\item We see how we can represent a generative model in a graphical format
\end{itemize}

	\item How can we use what we learned today?
\begin{itemize}
	\item We can think about how datasets we use might have been generated
\end{itemize}
\end{itemize}
\end{frame}

\end{document}

