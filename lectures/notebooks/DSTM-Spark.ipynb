{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark Notebook\n",
    "\n",
    "The following code sets up the \"Spark Context\" which is how we interact with Spark from Python. \n",
    "\n",
    "Useful Spark Python documentation can be found here: \n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\t\n",
    "https://spark.apache.org/docs/latest/api/python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:16.875641Z",
     "start_time": "2018-09-18T15:47:16.761572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to stop the context when you are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:14.181259Z",
     "start_time": "2018-09-18T15:47:13.180929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop() #commented out so that you don't stop your context by mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RDD, using the parallelize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:18.996519Z",
     "start_time": "2018-09-18T15:47:18.982578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "myRDD = sc.parallelize (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happend? Nothing was returned.\n",
    "\n",
    "We have to ask Spark to give us back the processed data.\n",
    "\n",
    "One function to do this is **collect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:19.472275Z",
     "start_time": "2018-09-18T15:47:19.414280Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myData = myRDD.collect()\n",
    "myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of object did we get back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function is **first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myData = myRDD.first()\n",
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take** returns the first _n_ elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myData = myRDD.take(2)\n",
    "myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another is **top**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myData = myRDD.top(3)\n",
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an RDD from a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:20.380045Z",
     "start_time": "2018-09-18T15:47:20.371812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDDrange = sc.parallelize (range (20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements are in the RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDDrange.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:26.582588Z",
     "start_time": "2018-09-18T15:47:26.512013Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDDrange.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create an RDD from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:44.398756Z",
     "start_time": "2018-09-18T15:47:44.371939Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md = sc.textFile ('./data/Moby-Dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check using one of our command line tools, word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wc -l ./data/Moby-Dick.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:46.131290Z",
     "start_time": "2018-09-18T15:47:46.044424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create an RDD from another RDD\n",
    "The classic example for this is word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:29.331011Z",
     "start_time": "2018-09-18T15:47:29.319535Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords (fileName):\n",
    "     lines = sc.textFile (fileName)\n",
    "     tokens = lines.flatMap (lambda line: line.split(\" \"))\n",
    "     instances = tokens.map (lambda word: (word, 1))\n",
    "     aggCounts = instances.reduceByKey (lambda a, b: a + b)\n",
    "     return aggCounts.top (200, key=lambda p: p[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are all sorts of transformations here:\n",
    "\n",
    "**flatMap**\n",
    "\n",
    "**map**\n",
    "\n",
    "**reduceByKey**\n",
    "\n",
    "We will walk through each of them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, a discussion about \n",
    "## lambdas\n",
    "\n",
    "* Basically, a function that that we can pass like a variable\n",
    "* Key ability: can **capture** its surroundings at creation\n",
    "* Can also accept parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:33.667073Z",
     "start_time": "2018-09-18T15:47:33.663252Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addTwelveToResult (myLambda):\n",
    "     return myLambda (3) + 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up some variables and a lambda to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:34.106418Z",
     "start_time": "2018-09-18T15:47:34.099628Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 23 # this is being captured\n",
    "aCoolLambda = lambda x : x + a\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(aCoolLambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we get when we call ``addTwelveToResult``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addTwelveToResult (aCoolLambda) # prints 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38? Why not 35? I'm adding 12 to ``a``, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:34.594874Z",
     "start_time": "2018-09-18T15:47:34.589528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "38-23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we added 15 instead. Why?\n",
    "\n",
    "The key here is that the function ``addTwelveToResult`` adds 12 to the value returned by calling the specified lambda with the value 3.\n",
    "\n",
    "So, basically, we're calling:\n",
    "\n",
    "    myLambda (3) + 12\n",
    "    (x + a) + 12\n",
    "    (3 + a) + 12\n",
    "    (3 + 23) + 12 \n",
    "\n",
    "which equals 38\n",
    "\n",
    "**The parenthesis matter!**\n",
    "\n",
    "To be sure we understand, let's do another example, but first set ``a = 45``. \n",
    "\n",
    "What should we get?\n",
    "\n",
    "Type your guess in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:35.293626Z",
     "start_time": "2018-09-18T15:47:35.285802Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=45\n",
    "addTwelveToResult (aCoolLambda) # prints ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Can we add another parameter to the function, so we can use something besides 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addTwelveToResultB (myLambda,b):\n",
    "     return myLambda (b) + 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to pass in 5 this time. Since a = 45 still, I should get 62.\n",
    "\n",
    "Let's check the value of **a** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addTwelveToResultB (aCoolLambda, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, we got 62.\n",
    "\n",
    "Anytime we see ``myLambda`` we replace it with the body of the lambda \n",
    "\n",
    "It's kind of like a database VIEW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Lambdas can return many items\n",
    "* Lambdas MUST return something\n",
    "\n",
    "What does this lambda return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:39.895964Z",
     "start_time": "2018-09-18T15:47:39.891194Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sumThem (myLambda):\n",
    "     tot = 0\n",
    "     for a in myLambda ():\n",
    "          tot = tot + a\n",
    "     return tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:40.299113Z",
     "start_time": "2018-09-18T15:47:40.292005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "iter = lambda : (j for j in x)\n",
    "sumThem (iter) # prints 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we want to pass in the square of each element of the array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 + 4 + 9 + 16 + 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = lambda : (j * j for j in x)\n",
    "sumThem (iter) # prints \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``sumThem`` will only work on items that are 'summable'. What happens if we try it on an array of strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(['a', 'b', 'c'])\n",
    "iter = lambda : (j for j in x)\n",
    "sumThem (iter) # error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't think that would work, but now I know. \n",
    "\n",
    "How could we fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sumThem (myLambda):\n",
    "     tot = ''\n",
    "     for a in myLambda ():\n",
    "          tot = tot + a\n",
    "     return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumThem (iter) # works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the code that motivated the discussion about lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords (fileName):\n",
    "     lines = sc.textFile (fileName)\n",
    "     tokens = lines.flatMap (lambda line: line.split(\" \"))\n",
    "     instances = tokens.map (lambda word: (word, 1))\n",
    "     aggCounts = instances.reduceByKey (lambda a, b: a + b)\n",
    "     return aggCounts.top (200, key=lambda p: p[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider just the first few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:47:43.705538Z",
     "start_time": "2018-09-18T15:47:43.701657Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords (fileName):\n",
    "     lines = sc.textFile (fileName)\n",
    "     tokens = lines.flatMap (lambda line: line.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does flatMap do?\n",
    "* Processes every data item in the RDD\n",
    "* Apply lambda to it\n",
    "* Lambda argument will return zero or more results\n",
    "* Can omit, combine or create elements\n",
    "* Each result is added into the resulting RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture here of transformation of RDD that is not 1-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the flatMap function on our text file. Note the use of the **split** function that breaks apart the line at spaces, tokenizing it. What do we exepect to get back? \n",
    "\n",
    "We are breaking lines of the book Moby Dick at spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:48:17.970074Z",
     "start_time": "2018-09-18T15:48:17.966870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = md.flatMap (lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:48:25.215585Z",
     "start_time": "2018-09-18T15:48:25.090812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens.top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line of our function is a call to map. \n",
    "Map:\n",
    "* Processes every data item in the RDD\n",
    "* Apply lambda to it\n",
    "* The lambda must return exactly one result\n",
    "* The returned RDD has a new element with each element replaced by the lamdba applied to the orginal term\n",
    "\n",
    "What do we think it will do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture here of transformation of RDD that is 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:49:11.363661Z",
     "start_time": "2018-09-18T15:49:11.358607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = tokens.map (lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords (fileName):\n",
    "     lines = sc.textFile (fileName)\n",
    "     tokens = lines.flatMap (lambda line: line.split(\" \"))\n",
    "     instances = tokens.map (lambda word: (word, 1))\n",
    "     aggCounts = instances.reduceByKey (lambda a, b: a + b)\n",
    "     return aggCounts.top (200, key=lambda p: p[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:49:32.643894Z",
     "start_time": "2018-09-18T15:49:32.531876Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We are getting closer to our goal of getting a count of all the tokens or 'words' in our book.\n",
    "\n",
    "Next, we want to matching tokens and sum the totals. \n",
    "\n",
    "We do this with the 'reduceByKey' function, which, also, takes a lambda. No surprise there. This time, our lamdba sums together values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next line is a call to **reduceByKey**\n",
    "* Data must be $(Key, Value)$ pairs\n",
    "* Shuffle so that all $(K, V)$ pairs with same $K$ on same machine\n",
    "* Organize into $(K, (V_1, V_2, ..., V_n))$ pairs\n",
    "* Use the lambda to **reduce** the list to a single value\n",
    "\n",
    "This is similar to our aggregate functions in SQL!\n",
    "\n",
    "What do we think the result of this function will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:49:43.913539Z",
     "start_time": "2018-09-18T15:49:43.862673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggCounts = instances.reduceByKey (lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? We might have to look at more items to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T15:49:55.160565Z",
     "start_time": "2018-09-18T15:49:55.048999Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggCounts.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside: Note that we are counting 'tokens' not words. We could have eliminated punctuation and converted everything to lowercase. That might have given us a more accurate count.  It depends on what we want to count.\n",
    "\n",
    "Since we now know that all the pieces work, we can put them together in a function. \n",
    "\n",
    "Note that this function returns the top 200 tokens in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we call **top** in our **countWords** function to return the top 200 words.\n",
    "\n",
    "What do you think the most common word(s) in the book will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggCounts.top (200, key=lambda p: p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Spark uses lazy evaluation...\n",
    "If we run this code\n",
    "```\n",
    "lines = sc.textFile (fileName)\n",
    "tokens = lines.flatMap (lambda line: line.split(\" \"))\n",
    "instances = tokens.map (lambda word: (word, 1))\n",
    "aggCounts = instances.reduceByKey (lambda a, b: a + b)\n",
    "```\n",
    "\n",
    "Nothing happens! (Other than Spark remembers the ops)\n",
    "* Spark does not execute until an attempt made to collect an RDD\n",
    "* When we hit **top()**, then all of these are executed\n",
    "Why does Spark do this?\n",
    "* By waiting until last possible second, we can  **pipeline** \n",
    "* Only operations that require a shuffle can't be pipelined\n",
    "\n",
    "Remember our terminal pipe command: |?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the whole function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countWords('./data/Moby-Dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem reasonable. There are a lot of articles, which are common in English. Moby Dick, if you didn't know, is about a whale. So the word whale appears as well. It's also about a ship captain named Ahab. The very famous first line of the book is \"Call me Ishmael.\" So, this book is likely told from his perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through some other, useful, pySpark functions.\n",
    "\n",
    "Let's define some new, small RDDs to use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddOne = {('red', 9), ('blue', 7), ('red', 12), ('green', 4)}\n",
    "myRddOne = sc.parallelize (rddOne)\n",
    "myRddOne.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rddTwo = {('blue', 'up'), ('green', 'down'), ('green', 'behind')}\n",
    "myRddTwo = sc.parallelize (rddTwo)\n",
    "myRddTwo.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## groupByKey()\n",
    "* Data must be $(Key, Value)$ pairs\n",
    "* Shuffle so that all $(K, V)$ pairs with same $K$ onto the same machine\n",
    "* Organize into $(K, \\langle V_1, V_2, ..., V_n \\rangle)$ pairs\n",
    "* Store each list as a ``ResultIterable`` for future processing\n",
    "* Like ``reduceByKey()`` but without the reduce\n",
    "\n",
    "\n",
    "``groupByKey`` returns pairs where the key is the key from the items in the original RDD and the value is a list of all the values for the matching keys.\n",
    "\n",
    "To see the results, we need to ``collect`` the list of values into something we can read. One possibility is to apply a ``map`` where we return a pair with the key and a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myGroupBy = myRddOne.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()\n",
    "myGroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function is a join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myJoin = myRddOne.join(myRddTwo)\n",
    "myJoin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the ``reduce`` function.\n",
    " \n",
    " This function is like ``top``. It returns a result back to Python.\n",
    " \n",
    " It repeatedly applies a lambda to each item in the RDD to get a single result.\n",
    " \n",
    " Here, we use it to sum all of the items in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myData = sc.parallelize (range(20000))\n",
    "myData.reduce (lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate\n",
    "\n",
    "One last aggregation example, the ``aggregate`` function.\n",
    "\n",
    "``aggregate`` takes 3 arguments: a 'zero' to initialize the aggregation and two lambdas. \n",
    "\n",
    "Lambda 1: takes two arguments, $X_1$ and $X_2$ and aggregates them, where $X_1$ is already aggregated, $X_2$ not.\n",
    "\n",
    "Lambda 2: takes $X_1$, $X_2$ and aggregates them, where both are already aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda 1: Define ``add`` that takes a dictionary and a tuple and returns a new dictionary that contains the staring dictionary and the new tuple\n",
    "\n",
    "(1) initialize the result dictionary\n",
    "\n",
    "(2) For each item in the dictionary we are given\n",
    "\n",
    "    (3)     Create an entry in our result dictionary with dict's value\n",
    "\n",
    "(4) If we've seen the tuple key before\n",
    "    \n",
    "    (5)     Accumulate the value\n",
    "\n",
    "(6) If this is the first time we see the tuple key\n",
    "\n",
    "     (7)     Create an entry for it and initialize with the value\n",
    "\n",
    "(8) Return our result dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add (dict, tuple):\n",
    "  result = {}\n",
    "  for key in dict:\n",
    "    result[key] = dict[key]\n",
    "  if (tuple[0] in result):\n",
    "    result[tuple[0]] += tuple[1]\n",
    "  else:\n",
    "    result[tuple[0]] = tuple[1]\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda 2: Define ``combine`` that takes 2 dictionaries and returns a new dictionary that contains alls the keys in both dictionaries, with the total counts\n",
    "\n",
    "(1) initialize the result dictionary\n",
    "\n",
    "(2) For each item in dict1 that we are given\n",
    "    \n",
    "    (3) Create an entry in our result dictionary with dict1's value\n",
    "\n",
    "(4) For each key, value in dict2 that we are given\n",
    "    \n",
    "    (5) If we've seen the key before\n",
    "        \n",
    "        (6) Accumulate the value\n",
    "    \n",
    "    (7) If this is the first time we see the tuple key\n",
    "    \n",
    "        (8) Create an entry for it and initialize with the value\n",
    "(9) Return our result dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine (dict1, dict2):\n",
    "  result = {}\n",
    "  for key in dict1:\n",
    "    result[key] = dict1[key]\n",
    "  for key in dict2:\n",
    "    if (key in result):\n",
    "      result[key] += dict2[key]\n",
    "    else:\n",
    "      result[key] = dict2[key]\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRdd = sc.parallelize ([('red', 9), ('blue', 7),  ('red', 12), ('green', 4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " myRdd.aggregate ({}, lambda x, y: add (x, y), lambda x, y: combine (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing thoughts\n",
    "* When is Spark/MapReduce a better option than HPC? \n",
    "    * When your pipeline is heavily data-oriented\n",
    "    * When your compute is (relatively) loosely coupled\n",
    "* Key benefits compared to HPC \n",
    "    * Built in fault tolerance\n",
    "    * Better support for BIG data\n",
    "    * Much higher programmer productivity\n",
    "* Will continue to take market share from HPC\n",
    "    * You see academic papers with both MPI, Spark implementations \n",
    "    * But not everything can move to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we use what we learned today?\n",
    "\n",
    "### What do we know now that we didn't know before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to stop your Spark context!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ©2019 Christopher M Jermaine (cmj4@rice.edu), and Risa B Myers  (rbm2@rice.edu)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
