{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this lab is to get some experience with Spark. \n",
    "\n",
    "The following code sets up the \"Spark Context\" which is how we interact with Spark from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS \n",
    " \n",
    "For the next lab, we will be repeating this activity, but on Amazon AWS.  This is Amazon’s computing-as-a-utility service. To prepare for Lab, you’ll need an AWS account.  Please do the following… preferably today!\n",
    "\n",
    " \n",
    "\n",
    "1) Sign up for an Amazon AWS account.  Go to aws.amazon.com to sign up if you don’t already have an account. Note, you will need a credit card to sign up, but there is no cost until you actually rent some machines to be activated.  Wait for your account to be activated.  This may take 24 hours.\n",
    "\n",
    " \n",
    "\n",
    "2) Optionally: once you have your account number, sign up for Amazon AWS credit here: https://aws.amazon.com/education/awseducate/apply/.  This will get you some free time on Amazon AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop() #commented out so that you don't stop your context by mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "Below is the countWords function from the Spark lecture.  We are going to run this code on a set of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords (fileName):\n",
    "    lines = sc.textFile(fileName)\n",
    "    tokens = lines.flatMap(lambda line: line.split(\" \"))\n",
    "    instances = tokens.map (lambda word: (word, 1))\n",
    "    aggregatedCounts = instances.reduceByKey (lambda a, b: a + b)\n",
    "    return aggregatedCounts.top (200, key=lambda p : p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the following files from the course Box folder and run countWords on each of them.\n",
    "\n",
    "https://rice.box.com/v/RiceDSToolsAndModels\n",
    "\n",
    "Holmes.txt\n",
    "\n",
    "war.txt\n",
    "\n",
    "william.txt\n",
    "\n",
    "dictionary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countWords(\"<path to your uploaded data here>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing a Spark Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the lab, we are going to use a subset of the 20 News groups dataset \n",
    "http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "The data file you need (20-news-same-line-small.txt) is also in the following folder:\n",
    "\n",
    "https://rice.box.com/v/RiceDSToolsAndModels\n",
    "\n",
    "This dataset is from back in the day when there were electronic \"bulletin board\" discussions on different topics on Usenet.\n",
    "\n",
    "Load the data into an RDD and take a peek at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('data/20-news-same-line-small.txt')\n",
    "myRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 19997 lines in this file, each\n",
    "corresponding to a different text document. The goal here is to build, as an RDD, a\n",
    "dictionary. The dictionary will have as its key a number from 0 to 19,999 (this is a rank)\n",
    "and the value is the word corresponding to that rank. The words will be ranked\n",
    "according to their frequency in the corpus, with 0 being the most frequent, 19,999 being\n",
    "the 20-thousandth-most frequent.\n",
    "\n",
    "Take a look at the code. \n",
    "\n",
    "Note that nothing interesting happens until you enter the line that tries to collect the\n",
    "results (using the call to top), at which point Spark goes off, performs all of the\n",
    "computations, and tries collect the results into topWords. At this point, topWords is a\n",
    "local variable. In your Jupyer Notebook, you can manipulate it, change it, and print it, just as\n",
    "you would any other local variable.\n",
    "\n",
    "Once you’ve made it to the question marks, come up with an appropriate lambda that is\n",
    "going to attach the correct word to each number, putting the results into an RDD.\n",
    "Replace the question marks with your lambda. The key of the tuple you create and put\n",
    "into the RDD via the lambda should be the word; the value is the index (the number\n",
    "from 0 to 19,999). Your lambda will refer to the local variable topWords.\n",
    "\n",
    "After you run dictionary.take(10) you can get checked off! Note that this will\n",
    "return the first 10 words by index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# load up all of the 19997 documents in the corpus\n",
    "corpus = sc.textFile (\"./data/20-news-same-line-small.txt\")\n",
    "\n",
    "# each entry in validLines will be a line from the text file\n",
    "validLines = corpus.filter(lambda x : 'id' in x)\n",
    "\n",
    "# now we transform it into a bunch of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "\n",
    "# now we split the text in each (docID, text) pair into a list of words\n",
    "# after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "# die on some of the documents\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "# now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: ((j, 1) for j in x[1]))\n",
    "\n",
    "# now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey (lambda a, b: a + b)\n",
    "\n",
    "# and get the top 20,000 words in a local array\n",
    "# each entry is a (\"word1\", count) pair\n",
    "topWords = allCounts.top (20000, lambda x : x[1])\n",
    "\n",
    "# and we'll create a RDD that has a bunch of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 thru 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "twentyK = sc.parallelize(range(20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Code Here\n",
    "\n",
    "Your task is to map the parallelized range $(0, 1, 2, \\ldots, )$ \n",
    "into a set of tuples (\"mostcommonword\", 0), (\"nextmostcommon\", 1), ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, we transform (0), (1), (2), ... to (\"mostcommonword\", 0) (\"nextmostcommon\", 1), ...\n",
    "# the number will be the spot in the dictionary used to tell us where the word is located\n",
    "# HINT: make use of topWords in the lambda that you supply\n",
    "dictionary = twentyK.map (?????????)\n",
    "\n",
    "# finally, print out some of the dictionary, just for debugging and to get checked off\n",
    "dictionary.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the Spark context when you are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop() #commented out so that you don't stop your context by mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ©2019 Christopher M Jermaine (cmj4@rice.edu), and Risa B Myers  (rbm2@rice.edu)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
