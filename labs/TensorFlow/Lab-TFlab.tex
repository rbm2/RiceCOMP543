
%Copyright ©2019 Christopher M Jermaine (cmj4@rice.edu), and Risa B Myers  (rbm2@rice.edu)
%
%Licensed under the Apache License, Version 2.0 (the "License");
%you may not use this file except in compliance with the License.
%You may obtain a copy of the License at
%
%    https://www.apache.org/licenses/LICENSE-2.0
%
%Unless required by applicable law or agreed to in writing, software
%distributed under the License is distributed on an "AS IS" BASIS,
%WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%See the License for the specific language governing permissions and
%limitations under the License.
\documentclass[11pt]{article}
%\documentclass[12pt]{amsart}
%\usepackage{latex8}
\usepackage{fullpage}
\usepackage{times}
\usepackage{url}
\usepackage[normalem]{ulem}
\usepackage{epsfig} 
%\usepackage{latexsym}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{multirow}

\titlespacing{\section}{0pt}{3mm}{1mm}
\titlespacing{\subsection}{0pt}{2mm}{0.5mm}
\titlespacing{\subsubsection}{0pt}{2mm}{0.8mm}

%\topmargin 0.75in 
%\oddsidemargin -0.04in
%\textwidth 6.5in
%\textheight 9.0in 
%\setlength{\textheight}{23.1cm}
%\setlength{\textwidth}{17.0cm}

\newcommand{\muhat}{\hat{\mu}}
\newcommand{\sigmahat}{\hat{\sigma}}
\newcommand{\todo}[1]{[\textbf{TODO: #1}]}
\newcommand{\eat}[1]{} % TO MAKE LARGE BLOCKS OF TEXT INVISIBLE
\newcommand{\sz}[1]{\lvert#1\rvert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\xp}[2]{P \if*#1\else^{#1}\fi \if*#2\else_{\! #2}\fi}
\newcommand{\pr}[3]{\xp{#1}{#2}\left\{\,#3\,\right\}}
\newcommand{\prl}[3]{\xp{#1}{#2}\{\,#3\,\}}
\renewcommand\:{\colon} % for use with \sset, etc.
\newcommand{\sset}[1]{\left\{\,#1\,\right\}}
\newcommand\xD{\mathcal{D}}
\newcommand\xP{\mathcal{P}}
\newcommand\xS{\mathcal{S}}
\newcommand\xbar{\bar x}
\newcommand\vbar{\bar v}
\newcommand\xmax{{x_{\text{max}}}}
\newcommand\eps{\epsilon}
\newcommand{\eeblk}{\hbox{\lower 1pt \vbox{\hrule width6pt\hbox to
  6pt{\vrule height5pt depth1pt \hfil\vrule height5pt depth1pt} \hrule
  width6pt} \unskip}}
\newcommand{\eblk}{{\unskip\nobreak\hfil\penalty50
  \hskip1em\hbox{}\nobreak\hfil\eeblk
  \parfillskip=0pt\finalhyphendemerits=0\par}}
\newtheorem{xample}{Example}
%\newenvironment{example}{\begin{xample}\em}{\eblk\end{xample}}
\makeatletter
\newenvironment{sql}%
 {\vskip 5pt\begin{list}{}{%
  \setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}\setlength{\parskip}{0pt}%
  \setlength{\parsep}{0pt}\setlength{\labelwidth}{0pt}%
  \setlength{\rightmargin}{0pt}\setlength{\leftmargin}{0pt}%
  \setlength{\labelsep}{0pt}%
  \obeylines\@vobeyspaces\normalfont\ttfamily%
  \item[]}}
 {\end{list}\vskip5pt\noindent}
\makeatother
\newcommand{\bpar}[1]{\vskip 5pt\noindent\textbf{#1}\hskip 1em}
\newcommand\yN{{\tilde N}}
\newcommand\yX{{\tilde X}}
\newcommand\ymu{{\tilde\mu}}
\newcommand\ysigma{{\tilde\sigma}}


\newcommand{\goodgap}{
        \hspace{\subfigtopskip}
        \hspace{\subfigbottomskip}
}

%\renewcommand{\baselinestretch}{0.99}

\newtheorem{definition}{Definition}
\newtheorem{Rule}{Rule}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{optimization}{Optimization}
\newtheorem{observation}{Observation}
\newtheorem{corollary}{Corollary}

\newcommand{\qed}{\hspace*{\fill}
           \vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}\smallskip}

\long\def \ignoreme#1{}

\def\qed{\hfill \mbox{\rule[0pt]{1.5ex}{1.5ex}}}



\begin{document}
%\maketitle
%\pagestyle{empty}

\begin{center}
{\bf \huge{Lab: Deep Learning with TensorFlow}}
\end{center}

\section{Description}

In this lab, you will be using Google's open-source TensorFlow machine learning tool to implement
a deep learning architectures that will be used to classify sequences of raw text (This time, no
pre-processing using dictionaries and words. We will be operating on raw characters!). Since deep learning
is computationally expensive, it is strongly recommended that you use one of Amazon's deep learning machines
to do this assignment (see below). The small learning problems we'll consider will take about 10-20
minutes max using one of these machines, but might take 10 times as long (or more) using a laptop. Plus,
TensorFlow can be a bit of a pain to install on your laptop, so using Amazon is just plain easier. The only
situation in which you might consider using your own hardware is if you've got a tricked out laptop/desktop
with a beefy GPU for game playing that TensorFlow can make use of.

\section{Preparation}

For this lab, you will need access to a GPU machine on AWS. 

Go to the AWS support center and create a case. 

\begin{enumerate}
\item For ``Regarding", select ``Service Limit Increase". 
\item For ``Limit Type", select ``EC2 instance". 
\item For ``region", select ``US East (N. Virginia)". 
\item For ``Primary Instance Type", select ``p2.xlarge". 
\item[] Note that, in p2.xlarge, there is a Tesla K80 GPU, which is sufficient for personal use. 
\item For New limit value enter 1
\end{enumerate}


To use:
\begin{enumerate}
\item If you would like to use more than one GPU at the same time, you may want to use p2.x8large.
\item Go back to the console, click ``Service" then click "EC2"
\item Click the blue button ``launch instance".
\item In the AMI webpage, select ``Deep Learning AMI".
\item Choose p2.xlarge on the next page, then click ``Review and Launch"
\item Create or use existing key.
\end{enumerate}

After some minutes, your EC2 instance is ready. \\

\begin{enumerate}
\item Go back to the console, click ``Service" 
\item Then click ``EC2"
\item Then click ``Instance" on the left side. 
\item Select the instance you created
\item Click ``connect"
\end{enumerate}

Note that if the region where the files are located is different than the region in which you have been working in the past, you will need to create a new .pem file in the new region. Once you download the new .pem file, be sure to change the permissions on it to owner only: \texttt{chmod 700 myNewFile.pem}.

\section{The Task: Running RNN Learning Using TensorFlow}

For this task, you'll make a slight modification to a Python TensorFlow code provided, and then
you'll run it on EC2 (this is Amazon's computer rental service; you already have experience running EC2,
although previously you used EC2 via Amazon's EMR service; you didn't start up machines using EC2
directly).

To get started, log on to Amazon AWS, then click on ``EC2'', and ``Launch Instance''. You will be asked
for a machine instance to start up (this will govern the software on the machine you run). Scroll down to
``Deep Learning AMI (Ubuntu) Version [X] - ami-[id]'' and click ``Select''.  Make sure you select a Deep Learning AMI, not a Deep Learning Base AMI. This machine instance (not
surprisingly, given the name) has a number of deep learning tools installed on it. Next you need to choose
the machine type you will rent. You will want to choose a machine with a GPU, which will make your deep
learning codes much faster. Choose ``p2.xlarge``, then ``Next: Configure Instance Details``, please make sure that you set IAM role as ``EMR\_EC2\_DefaultRole``, then ``Review and Launch''. You want to make sure that you
can SSH into your machine, so choose ``Edit Security Groups'' then click ``Add Rule'' to make sure to allow
SSH access. Once you have done that, click ``Review and Launch'' and then ``Launch''. You can find your
machine by going to the EC2 Dashboard and then clicking on ``Running Instances''.

As usual, when you are done with your machine, \textbf{MAKE SURE TO SHUT IT DOWN!!}

Once you have a machine up and running, SSH into your machine (just like for EMR) then load up three
of the data sets from the first Lab:\\
\texttt{aws s3 cp s3://[DataLocation]/Holmes.txt ./Holmes.txt}\\
\texttt{aws s3 cp s3://[DataLocation]/war.txt ./war.txt}\\
\texttt{aws s3 cp s3://[DataLocation]/william.txt ./william.txt}\\

Our goal is to implement a deep learner that is able to accept lines of text from each of those three files
and classify the line correctly (that is, accurately determine which file the line came from). To do this, we
will fire up Python since TensorFlow has a Python API. Start by typing:\\

\texttt{ubuntu@ip-172-16-0-163: source activate tensorflow\_p36}\\

\noindent This will give you a virtual environment to run TensorFlow via Python. Next:\\
\texttt{(tensorflow\_p36) ubuntu@ip-172-16-0-163: python}


\noindent This will fire up Python. At this point, you will want to run the code provided. This code implements
an RNN (a classic RNN that does not use LSTM) that tries to determine what file each line of text came
from, by only looking at the sequence of characters. Look over the code before you run it. The supplied
code will first load up data from the three files, and then it will run the backprop learning algorithm for
10,000 iterations, where each iteration processes a ``mini-batch'' that is a set of 100 randomly-selected lines
from the three files. Learning (all 10,000 iterations) might take 10 minutes on the p2.xlarge machine.

Once you've run the learning algorithm (or after you've gotten sick of watching it for a few minutes),
your task is to slightly modify the Python code I've provided so that at the end of learning, it says, ``Average
loss for the last 10 mini-batches is 0.9982, average correct labels is 56.5 out of 100''. Naturally, these values
should not be hard-coded, they should be computed to represent the actual observed averages.

Once you have made this modification, run the code for the full 10,000 iterations. To get checked off, show 
the output for the last 20 iterations, plus the message that you added,  to a TA/Instructor.


\end{document}
